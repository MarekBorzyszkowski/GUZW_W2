{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqEekjg8cIOZ"
   },
   "source": [
    "# Definition of used classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:38:23.746444Z",
     "start_time": "2025-01-17T02:38:23.364266Z"
    },
    "id": "Qn-zMldmcODa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import numpy as np\n",
    "import cv2\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Video\n",
    "\n",
    "from pygame import Vector2 as Vec2\n",
    "from pygame import Color as Color\n",
    "\n",
    "# Constants\n",
    "FPS: int = 30\n",
    "SCREEN_SIZE: Vec2 = Vec2(640, 440)\n",
    "PITCH_SIZE: Vec2 = Vec2(600, 400)\n",
    "PITCH_POS: Vec2 = (SCREEN_SIZE - PITCH_SIZE) / 2\n",
    "POST_TOP_Y: float = PITCH_POS.y + PITCH_SIZE.y * 6 / 16\n",
    "POST_BOT_Y: float = PITCH_POS.y + PITCH_SIZE.y * 10 / 16\n",
    "LINE_WIDTH: int = 2\n",
    "\n",
    "# Color palette\n",
    "COLOR_WHITE: Color = Color(255, 255, 255)\n",
    "COLOR_BLACK: Color = Color(0, 0, 0)\n",
    "COLOR_LINE: Color = Color(174, 202, 137)\n",
    "COLOR_PITCH: Color = Color(113, 152, 63)\n",
    "COLOR_BACKGROUND: Color = Color(91, 143, 164)\n",
    "\n",
    "\n",
    "class Drawable(ABC):\n",
    "    @abstractmethod\n",
    "    def draw(self, img):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class CirclePhysical(ABC):\n",
    "    COUNT: int = 0\n",
    "\n",
    "    def __init__(self, radius: float, mass: float, pos: Vec2, hooked: bool = False):\n",
    "        self.id: int = CirclePhysical.COUNT\n",
    "        CirclePhysical.COUNT += 1\n",
    "\n",
    "        self.radius: float = radius\n",
    "        self.mass: float = mass\n",
    "        self.friction: float = 0.7 * mass\n",
    "        self.v_max = 12 / math.pow(mass, 2 / 3)\n",
    "        self.pos: Vec2 = pos\n",
    "        self.vel: Vec2 = Vec2(0, 0)\n",
    "        self.hooked: bool = hooked\n",
    "\n",
    "    def setPos(self, pos: Vec2):\n",
    "        self.pos = pos\n",
    "\n",
    "    def setVel(self, vel: Vec2):\n",
    "        self.vel = vel * self.mass\n",
    "\n",
    "    def addVel(self, vel: Vec2):\n",
    "        self.vel += vel\n",
    "\n",
    "    def update(self, dt: float):\n",
    "        self.pos += self.vel * dt\n",
    "\n",
    "        self.vel += -self.vel * self.friction * dt\n",
    "        self.pos += self.vel * dt\n",
    "\n",
    "        if self.vel.magnitude() > self.v_max:\n",
    "            self.vel = self.vel.normalize() * self.v_max\n",
    "\n",
    "    @staticmethod\n",
    "    def sphereCollisionVelocities(\n",
    "            v1: Vec2, v2: Vec2,\n",
    "            m1: float, m2: float,\n",
    "            x1: Vec2, x2: Vec2) -> Tuple[Vec2, Vec2]:\n",
    "        # Calculate new velocities after collision\n",
    "        mass: float = 2 * m1 / (m1 + m2)\n",
    "        v1_new: Vec2 = v1 - (mass * (v1 - v2).dot(x1 - x2) / pow((x1 - x2).length(), 2)) * (x1 - x2)\n",
    "        v2_new: Vec2 = v2 - (mass * (v2 - v1).dot(x2 - x1) / pow((x2 - x1).length(), 2)) * (x2 - x1)\n",
    "        return v1_new, v2_new\n",
    "\n",
    "    def collide(self, other: 'CirclePhysical'):\n",
    "        dist = self.pos.distance_to(other.pos)\n",
    "        if dist < self.radius + other.radius:\n",
    "            if dist == 0:\n",
    "                self.pos += -self.vel.normalize() * 0.01\n",
    "\n",
    "            self.vel, other.vel = CirclePhysical.sphereCollisionVelocities(\n",
    "                self.vel, other.vel, self.mass, other.mass, self.pos, other.pos)\n",
    "\n",
    "            if not self.hooked:\n",
    "                self.pos += (self.pos - other.pos).normalize() * (self.radius + other.radius - dist) / 2\n",
    "            else:\n",
    "                self.vel = Vec2(0, 0)\n",
    "\n",
    "            if not other.hooked:\n",
    "                other.pos += (other.pos - self.pos).normalize() * (self.radius + other.radius - dist) / 2\n",
    "            else:\n",
    "                other.vel = Vec2(0, 0)\n",
    "\n",
    "\n",
    "class Ball(CirclePhysical, Drawable):\n",
    "    COLOR: Color = Color(255, 255, 255)\n",
    "    MASS: float = 0.2\n",
    "    RADIUS: float = 7\n",
    "\n",
    "    def __init__(self, pos: Vec2 = Vec2(SCREEN_SIZE.x / 2, SCREEN_SIZE.y / 2)):\n",
    "        super().__init__(\n",
    "            radius=self.RADIUS, mass=self.MASS, pos=pos)\n",
    "\n",
    "    def draw(self, img):\n",
    "        cv2.circle(img, (int(self.pos.x), int(self.pos.y)), self.radius, self.COLOR, -1)\n",
    "\n",
    "\n",
    "class Player(CirclePhysical, Drawable):\n",
    "    MASS: float = 0.5\n",
    "    RADIUS: float = 10\n",
    "\n",
    "    def __init__(self, pos: Vec2, color: Color):\n",
    "        super().__init__(\n",
    "            radius=self.RADIUS, mass=self.MASS, pos=pos)\n",
    "\n",
    "        self.color: Color = color\n",
    "\n",
    "    def draw(self, img):\n",
    "        cv2.circle(img, (int(self.pos.x), int(self.pos.y)), self.radius, self.color, -1)\n",
    "\n",
    "\n",
    "class Post(CirclePhysical, Drawable):\n",
    "    MASS: float = 1\n",
    "    RADIUS: float = 6\n",
    "\n",
    "    def __init__(self, pos: Vec2, color: Color):\n",
    "        super().__init__(\n",
    "            radius=self.RADIUS, mass=self.MASS, pos=pos, hooked=True)\n",
    "\n",
    "        self.color: Color = color\n",
    "\n",
    "    def draw(self, img):\n",
    "        cv2.circle(img, (int(self.pos.x), int(self.pos.y)), self.radius, self.color, -1)\n",
    "\n",
    "\n",
    "class Goal(Drawable):\n",
    "    def __init__(self, dir: int, linePos: float, postTopY: float, postBotY: float, color: Color):\n",
    "        self.dir: int = dir\n",
    "        self.linePos: float = linePos\n",
    "        self.yTop: float = postTopY\n",
    "        self.yBot: float = postBotY\n",
    "        self.color: Color = color\n",
    "\n",
    "        self.postTop: Post = Post(Vec2(linePos, postTopY), color)\n",
    "        self.postBot: Post = Post(Vec2(linePos, postBotY), color)\n",
    "\n",
    "    def draw(self, img):\n",
    "        cv2.line(img, (int(self.linePos), int(self.yTop)), (int(self.linePos), int(self.yBot)), self.color, LINE_WIDTH)\n",
    "        self.postTop.draw(img)\n",
    "        self.postBot.draw(img)\n",
    "\n",
    "\n",
    "class Team(Drawable):\n",
    "    def __init__(self, dir: int, color: Color):\n",
    "        self.color: Color = color\n",
    "\n",
    "        # Dir is 1 or -1\n",
    "        # 1 is attacking from left to right\n",
    "        # -1 is attacking from right to left\n",
    "        self.dir: int = dir\n",
    "\n",
    "        # Normalized dir is 0 or 1\n",
    "        # 0 is attacking from left to right\n",
    "        # 1 is attacking from right to left\n",
    "        self.normalizedDir: int = -(dir - 1) / 2\n",
    "\n",
    "        # Goal line for dir 1 is pitchPos.x\n",
    "        # Goal line for dir -1 is pitchPos.x + pitchSize.x\n",
    "        goalLineX: float = PITCH_POS.x + PITCH_SIZE.x * self.normalizedDir\n",
    "\n",
    "        self.goal: Goal = Goal(dir, goalLineX, POST_TOP_Y, POST_BOT_Y, color)\n",
    "        self.player: Player = Player(Vec2(goalLineX, SCREEN_SIZE.y / 2), color)\n",
    "\n",
    "        self.score: int = 0\n",
    "\n",
    "    def draw(self, img):\n",
    "        self.goal.draw(img)\n",
    "        self.player.draw(img)\n",
    "\n",
    "    def reset(self):\n",
    "        self.score = 0\n",
    "        self.player.setPos(\n",
    "            Vec2(self.goal.linePos + self.dir * PITCH_SIZE.x / 4, SCREEN_SIZE.y / 2 + self.dir * SCREEN_SIZE.y / 6))\n",
    "        self.player.setVel(Vec2(0, 0))\n",
    "\n",
    "    def applyAction(self, action: Vec2):\n",
    "        # Convert action to direction and invert it in relation to team dir\n",
    "        if action.length() != 0:\n",
    "            vel: Vec2 = action.normalize() * self.dir\n",
    "\n",
    "            # Apply velocity to player\n",
    "            self.player.addVel(vel)\n",
    "\n",
    "\n",
    "class Pitch(Drawable):\n",
    "    def __init__(self):\n",
    "        self.team1: Team = Team(1, Color(255, 0, 0))\n",
    "        self.team2: Team = Team(-1, Color(0, 0, 255))\n",
    "\n",
    "    def draw(self, img):\n",
    "        # Draw pitch as a solid color filled rectangle with a line border\n",
    "        cv2.rectangle(img, (int(PITCH_POS.x), int(PITCH_POS.y)),\n",
    "                      (int(PITCH_POS.x + PITCH_SIZE.x), int(PITCH_POS.y + PITCH_SIZE.y)), COLOR_PITCH, -1)\n",
    "        cv2.rectangle(img, (int(PITCH_POS.x), int(PITCH_POS.y)),\n",
    "                      (int(PITCH_POS.x + PITCH_SIZE.x), int(PITCH_POS.y + PITCH_SIZE.y)), COLOR_LINE, LINE_WIDTH)\n",
    "\n",
    "        # Draw center line\n",
    "        cv2.line(img, (int(PITCH_POS.x + PITCH_SIZE.x / 2), int(PITCH_POS.y)),\n",
    "                 (int(PITCH_POS.x + PITCH_SIZE.x / 2), int(PITCH_POS.y + PITCH_SIZE.y)), COLOR_LINE, LINE_WIDTH)\n",
    "        cv2.circle(img, (int(PITCH_POS.x + PITCH_SIZE.x / 2), int(PITCH_POS.y + PITCH_SIZE.y / 2)),\n",
    "                   int(PITCH_SIZE.y / 6), COLOR_LINE, LINE_WIDTH)\n",
    "\n",
    "        # Draw goals\n",
    "        self.team1.draw(img)\n",
    "        self.team2.draw(img)\n",
    "\n",
    "    def pitchCollision(self, sphere: CirclePhysical, dt: float) -> int:\n",
    "        # Check sphere's collision with pitch\n",
    "        # Return 0 if nothing happens\n",
    "        # Return 1 if team sphere is in goal of team 1\n",
    "        # Return 2 if team sphere is in goal of team 2\n",
    "\n",
    "        # Check if the sphere collides with the pitch's top or bottom\n",
    "        if (sphere.pos.y - sphere.radius < PITCH_POS.y or\n",
    "                sphere.pos.y + sphere.radius > PITCH_POS.y + PITCH_SIZE.y):\n",
    "            sphere.pos.y += -sphere.vel.y * dt\n",
    "            sphere.vel.y *= -1\n",
    "\n",
    "        # Check if the sphere collides with the goal\n",
    "        if (sphere.pos.y > POST_TOP_Y and sphere.pos.y < POST_BOT_Y) and isinstance(sphere, Ball):\n",
    "            # Check if the sphere collides with the goal of team 1\n",
    "            if (sphere.pos.x + sphere.radius < self.team1.goal.linePos):\n",
    "                return 1\n",
    "\n",
    "            # Check if the sphere collides with the goal of team 2\n",
    "            if (sphere.pos.x - sphere.radius > self.team2.goal.linePos):\n",
    "                return 2\n",
    "\n",
    "            return 0\n",
    "\n",
    "        # Check if the sphere collides with the pitch's left or right\n",
    "        if (sphere.pos.x - sphere.radius < PITCH_POS.x or\n",
    "                sphere.pos.x + sphere.radius > PITCH_POS.x + PITCH_SIZE.x):\n",
    "            sphere.pos.x += -sphere.vel.x * dt\n",
    "            sphere.vel.x *= -1\n",
    "\n",
    "        return 0\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer: List[Tuple(Vec2, Vec2, Vec2)] = []\n",
    "\n",
    "    def snapshot(self, ball: Ball, team1: Team, team2: Team):\n",
    "        self.buffer.append((ball.pos.copy(), team1.player.pos.copy(), team2.player.pos.copy()))\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = []\n",
    "\n",
    "\n",
    "class Haxball():\n",
    "    def __init__(self):\n",
    "        self.ball: Ball = Ball()\n",
    "        self.pitch: Pitch = Pitch()\n",
    "        self.player1: Player = self.pitch.team1.player\n",
    "        self.player2: Player = self.pitch.team2.player\n",
    "\n",
    "        self.circlePhysicals: List[CirclePhysical] = [\n",
    "            self.ball, self.player1, self.player2,\n",
    "            self.pitch.team1.goal.postTop, self.pitch.team1.goal.postBot,\n",
    "            self.pitch.team2.goal.postTop, self.pitch.team2.goal.postBot\n",
    "        ]\n",
    "\n",
    "        self.paused: bool\n",
    "        self.lastAction1: Vec2 = Vec2(0, 0)\n",
    "        self.lastAction2: Vec2 = Vec2(0, 0)\n",
    "\n",
    "        self.replayBuffer: ReplayBuffer = ReplayBuffer()\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.paused = False\n",
    "        self.lastAction1 = Vec2(0, 0)\n",
    "        self.lastAction2 = Vec2(0, 0)\n",
    "\n",
    "        # Reset ball\n",
    "        self.ball.setPos(Vec2(SCREEN_SIZE.x / 2, SCREEN_SIZE.y / 2))\n",
    "        self.ball.setVel(Vec2(0, 0))\n",
    "\n",
    "        # Reset teams\n",
    "        self.pitch.team1.reset()\n",
    "        self.pitch.team2.reset()\n",
    "\n",
    "        # Save first snapshot to replay buffer\n",
    "        self.replayBuffer.reset()\n",
    "        self.replayBuffer.snapshot(self.ball, self.pitch.team1, self.pitch.team2)\n",
    "\n",
    "    @staticmethod\n",
    "    def getRelativePos(pos: Vec2, dir: int) -> Vec2:\n",
    "        # Return player position normalized to the given dir\n",
    "        # (0, PITCH_SIZE.y / 2) is the center of own goal line\n",
    "\n",
    "        normalizedDir = -(dir - 1) / 2\n",
    "\n",
    "        return (pos - SCREEN_SIZE * normalizedDir) * dir\n",
    "\n",
    "    @staticmethod\n",
    "    def getRelativeVel(vel: Vec2, dir) -> Vec2:\n",
    "        # Return player velocity normalized to the given dir\n",
    "\n",
    "        return vel * dir\n",
    "\n",
    "    def info(self):\n",
    "        info = dict()\n",
    "        info[\"pitch_size\"] = (int(PITCH_SIZE.x), int(PITCH_SIZE.y))\n",
    "        info[\"pitch_pos\"] = (int(PITCH_POS.x), int(PITCH_POS.y))\n",
    "        info[\"screen_size\"] = (int(SCREEN_SIZE.x), int(SCREEN_SIZE.y))\n",
    "        info[\"player_radius\"] = Player.RADIUS\n",
    "        info[\"ball_radius\"] = Ball.RADIUS\n",
    "        info[\"post_radius\"] = Post.RADIUS\n",
    "        info[\"goal_width\"] = POST_BOT_Y - POST_TOP_Y\n",
    "        info[\"own_postTop_pos\"] = self.getRelativePos(self.pitch.team1.goal.postTop.pos, 1)\n",
    "        info[\"own_postBot_pos\"] = self.getRelativePos(self.pitch.team1.goal.postBot.pos, 1)\n",
    "        info[\"opp_postTop_pos\"] = self.getRelativePos(self.pitch.team2.goal.postTop.pos, 1)\n",
    "        info[\"opp_postBot_pos\"] = self.getRelativePos(self.pitch.team2.goal.postBot.pos, 1)\n",
    "        info[\"state_space\"] = 12\n",
    "        info[\"action_space\"] = 2\n",
    "        return info\n",
    "\n",
    "    def state(self):\n",
    "        return self.getState(1), self.getState(2), self.paused\n",
    "\n",
    "    def render(self):\n",
    "        img = np.zeros((int(SCREEN_SIZE.y), int(SCREEN_SIZE.x), 3), np.uint8)\n",
    "\n",
    "        cv2.rectangle(img, (0, 0), (int(SCREEN_SIZE.x), int(SCREEN_SIZE.y)), COLOR_BACKGROUND, -1)\n",
    "\n",
    "        self.pitch.draw(img)\n",
    "        self.ball.draw(img)\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def showReplay(self, custom_string=\"\", with_show_video=True):\n",
    "        video_path = f\"tmp_{custom_string}_{datetime.now().strftime('%d-%m-%y--%H-%M-%S')}.mp4\"\n",
    "\n",
    "        video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), FPS, (self.render().shape[1::-1]))\n",
    "\n",
    "        # Create video for whole replay buffer\n",
    "        for snapshot in self.replayBuffer.buffer:\n",
    "            # print(snapshot)\n",
    "            self.ball.pos = snapshot[0]\n",
    "            self.pitch.team1.player.pos = snapshot[1]\n",
    "            self.pitch.team2.player.pos = snapshot[2]\n",
    "\n",
    "            video.write(self.render())\n",
    "\n",
    "        video.release()\n",
    "        if with_show_video:\n",
    "            fileToDisplay = \"video.mp4\"\n",
    "            os.system(\n",
    "                f\"ffmpeg -loglevel level+fatal -y -i {video_path} -vcodec libx264 -x264opts keyint=123:min-keyint=120 -an {fileToDisplay}\")\n",
    "            display(Video(fileToDisplay, embed=True))\n",
    "        else:\n",
    "            print(video_path)\n",
    "\n",
    "    def getState(self, team: int):\n",
    "        dir: int\n",
    "        reward: int\n",
    "        lastAction: Vec2\n",
    "        player: Player\n",
    "        opponent: Player\n",
    "\n",
    "        if team == 1:\n",
    "            dir = 1\n",
    "            reward = self.pitch.team1.score\n",
    "            lastAction = self.lastAction1\n",
    "            player = self.player1\n",
    "            opponent = self.player2\n",
    "        elif team == 2:\n",
    "            dir = -1\n",
    "            reward = self.pitch.team2.score\n",
    "            lastAction = self.lastAction2\n",
    "            player = self.player2\n",
    "            opponent = self.player1\n",
    "        else:\n",
    "            raise Exception(\"Invalid team\")\n",
    "\n",
    "        ballPos: Vec2 = self.getRelativePos(self.ball.pos, dir)\n",
    "        ballVel: Vec2 = self.getRelativeVel(self.ball.vel, dir)\n",
    "        playerPos: Vec2 = self.getRelativePos(player.pos, dir)\n",
    "        playerVel: Vec2 = self.getRelativeVel(player.vel, dir)\n",
    "        opponentPos: Vec2 = self.getRelativePos(opponent.pos, dir)\n",
    "        opponentVel: Vec2 = self.getRelativeVel(opponent.vel, dir)\n",
    "\n",
    "        state: List[float] = [\n",
    "            ballPos.x, ballPos.y, ballVel.x, ballVel.y,\n",
    "            playerPos.x, playerPos.y, playerVel.x, playerVel.y,\n",
    "            opponentPos.x, opponentPos.y, opponentVel.x, opponentVel.y\n",
    "        ]\n",
    "\n",
    "        return state, reward, lastAction\n",
    "\n",
    "    def collide(self, sphere: CirclePhysical, dt: float) -> int:\n",
    "        # Collide with other spheres\n",
    "        for other in self.circlePhysicals:\n",
    "            if other != sphere:\n",
    "                sphere.collide(other)\n",
    "\n",
    "        # Collide with pitch\n",
    "        return self.pitch.pitchCollision(sphere, dt)\n",
    "\n",
    "    def step(self, action1: Vec2, action2: Vec2):\n",
    "        if self.paused:\n",
    "            self.replayBuffer.snapshot(self.ball, self.pitch.team1, self.pitch.team2)\n",
    "            return self.state()\n",
    "\n",
    "        dt: float = 15 / FPS\n",
    "\n",
    "        # Apply actions\n",
    "        self.pitch.team1.applyAction(action1)\n",
    "        self.pitch.team2.applyAction(action2)\n",
    "\n",
    "        # Update players\n",
    "        self.player1.update(dt)\n",
    "        self.player2.update(dt)\n",
    "        # Update ball\n",
    "        self.ball.update(dt)\n",
    "\n",
    "        # Check for collisions\n",
    "        self.collide(self.player1, dt)\n",
    "        self.collide(self.player2, dt)\n",
    "        res: int = self.collide(self.ball, dt)\n",
    "\n",
    "        # Save snapshot to replay buffer\n",
    "        self.replayBuffer.snapshot(self.ball, self.pitch.team1, self.pitch.team2)\n",
    "\n",
    "        self.lastAction1 = action1\n",
    "        self.lastAction2 = action2\n",
    "\n",
    "        # Check if a goal was scored\n",
    "        if res == 1:\n",
    "            # print(\"GOAL! for team 2\")\n",
    "            self.pitch.team2.score += 1\n",
    "            self.paused = True\n",
    "        elif res == 2:\n",
    "            self.pitch.team1.score += 1\n",
    "            # print(\"GOAL! for team 1\")\n",
    "            self.paused = True\n",
    "\n",
    "        return self.state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWyXSU5ecRSy"
   },
   "source": [
    "# The environment\n",
    "The environment was defined in the Haxball class.<br>\n",
    "\n",
    "### State\n",
    "State is returned by function `step(action1, action2)` or `state()`.\n",
    "State consists of 3 values: `state1, state2, done`:\n",
    "* `state1` -- `Tuple(state, reward, lastAction)`:\n",
    "  * `state` -- `Dict`: game state from the perspective of a player 1:\n",
    "    * `ballPos` -- `Vec2`: position of the ball\n",
    "    * `ballVel` -- `Vec2`: velocity of the ball\n",
    "    * `playerPos` -- `Vec2`: position of the player\n",
    "    * `playerVel` -- `Vec2`: velocity of the player\n",
    "    * `opponentPos` -- `Vec2`: position of the opponent\n",
    "    * `opponentVel` -- `Vec2`: velocity of the opponent\n",
    "  * `reward` -- `int`: reward for the last action\n",
    "  * `lastAction` -- `Vec2`: last action taken by the player\n",
    "* `state2` -- same as `state1` but from the perspective of a player 2\n",
    "* `done` -- `bool`: True if the game is over (goal was scored)\n",
    "\n",
    "\n",
    "### Info\n",
    "Function `info()` returns a dictionary containing information about the environment:\n",
    " * `pitch_size` -- size of the pitch\n",
    " * `pitch_pos` -- position of the top left corner of the screen\n",
    " * `screen_size` -- size of the whole screen (pitch + border)\n",
    " * `player_radius` -- radius of the player\n",
    " * `ball_radius` -- radius of the ball\n",
    " * `post_radius` -- radius of the post\n",
    " * `goal_width` -- width of the goal (y of the bottom post - y of the top post)\n",
    "\n",
    "### Action\n",
    "Action is continuous and is a `Vec2` type. It specifies the direction of the movement. It doesn't have to be normalized (magnitude doesn't affect the velocity).\n",
    "\n",
    "Action is relative to the player's goal. `Vec(1, 0)` (forward movement along x-axis) for one player means going to the right side of the screen, but for another it means going to the left (from the viewer's perspective).\n",
    "\n",
    "States are already properly reflected in relation to player's goal, so if you work on them, actions should work the some for both players - there is no difference which player you are using.\n",
    "\n",
    "`Vec2` is a `pygame.Vector2` type. You can look up it's documentation online. It supports basic vector operations.\n",
    "\n",
    "\n",
    "### Reset\n",
    "`reset()` resets the environment: score, all positions and velocities and replay buffer.\n",
    "\n",
    "### Replays\n",
    "`showReplay()` displays video of a last game (since the last reset).\n",
    "\n",
    "### Environment parameters\n",
    "On top of the code \"Constants\" section can be found. They can be used to modify the environment.\n",
    "You can change them to modify the environment:\n",
    "* `FPS` - number of frames per second - affects the video, but also the simulation (changes number of simulation steps)\n",
    "* `SCREEN_SIZE` - size of the screen - can't be smaller than pitch size - affects performance of video rendering\n",
    "* `PITCH_SIZE` - size of the pitch - affects the domain of the problem\n",
    "* `POST_TOP_Y/POST_BOT_Y` - width of the goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T21:14:15.010683Z",
     "start_time": "2025-01-16T21:14:15.003785Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52YmUr8RcYLj",
    "outputId": "d5eb3831-dbc3-4f57-ed85-35a0d6be524b"
   },
   "outputs": [],
   "source": [
    "env = Haxball()\n",
    "info = env.info()\n",
    "print(f\"info: {info}\\n\")\n",
    "\n",
    "state1, state2, done = env.state()\n",
    "print(f\"state1: {state1}\\n\")\n",
    "print(f\"state2: {state2}\\n\")\n",
    "print(f\"done:  {done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqsn34lLccZK"
   },
   "source": [
    "## Sample game simulation\n",
    "\n",
    "Below you can see a sample game simulation. Half of the moves are random and half are forward (notice that forward movement is different direction for each player!).\n",
    "\n",
    "Frames count is specified in `frames` variable. You can change it to see longer games. Length of the game (in seconds) is `frames / FPS`.\n",
    "\n",
    "After the game, replay is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T10:50:37.853818Z",
     "start_time": "2025-01-02T10:50:28.007548Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "2uqQeerYcdaU",
    "outputId": "33db8b78-b722-474c-de95-d8e4bf15e3d9"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def randomVector():\n",
    "    # Return a random vector from -1 to 1\n",
    "    return Vec2(random.random() * 2 - 1, random.random() * 2 - 1)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# Play a random game\n",
    "frames: int = 720\n",
    "\n",
    "for i in range(frames):\n",
    "    if random.random() < 0.5:\n",
    "        # Move randomly\n",
    "        state1, state2, done = env.step(randomVector(), randomVector())\n",
    "    else:\n",
    "        # Move forward\n",
    "        state1, state2, done = env.step(Vec2(15, 0), Vec2(1, 0))\n",
    "\n",
    "env.showReplay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n43MoTQgcisq"
   },
   "source": [
    "## Movement towards the ball\n",
    "\n",
    "Below you can see a different game simulation. In this game, players are always moving towards the ball. Notice how the direction towards the ball is calculated from state info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "AgkC1PwNcjsb",
    "outputId": "1cfa9136-8881-4579-cee9-ac756531436c"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "env = Haxball()\n",
    "\n",
    "# Play a random game\n",
    "frames: int = 1800\n",
    "\n",
    "for i in range(frames):\n",
    "    # Move agent towards the ball\n",
    "    player1State, reward, lastAction = env.getState(1)\n",
    "    player2State, reward, lastAction = env.getState(2)\n",
    "\n",
    "    ball1Pos: Vec2 = Vec2(player1State[0], player1State[1])\n",
    "    player1Pos: Vec2 = Vec2(player1State[4], player1State[5])\n",
    "\n",
    "    ball2Pos: Vec2 = Vec2(player2State[0], player2State[1])\n",
    "    player2Pos: Vec2 = Vec2(player2State[4], player2State[5])\n",
    "\n",
    "    diff1: Vec2 = (ball1Pos - player1Pos).normalize()\n",
    "    diff2: Vec2 = (ball2Pos - player2Pos).normalize()\n",
    "\n",
    "    state1, state2, done = env.step(diff1, diff2)\n",
    "\n",
    "env.showReplay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzNjp0plcnw9"
   },
   "source": [
    "# Zadanie wykładowe 2\n",
    "Stwórz wirtualnego gracza piłki nożnej sterowanego z wykorzystaniem sieci neuronowej. Wykorzystaj jeden z algortmów z wykładu siódmego: DQN, Double DQN lub Dueling DQN.\n",
    "\n",
    "# Zadania do wykonania\n",
    "* Opracuj sieć neuronową wyznaczającą wartość Q.\n",
    "* Zaimplementuj algorytm uczenia sieci neuronowej.\n",
    "* Naucz piłkarza dryblowania, czyli prowadzenia piłki do wskazanego punktu.\n",
    "* Naucz piłkarza omijania przeciwnika i prowadzenia piłki do bramki.\n",
    "* Przeprowadź gry między opracowanymi graczami.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:38:29.027820Z",
     "start_time": "2025-01-17T02:38:26.211466Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from pygame.math import Vector2 as Vec2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:38:29.046584Z",
     "start_time": "2025-01-17T02:38:29.039918Z"
    }
   },
   "outputs": [],
   "source": [
    "cos_22dot5 = 1 / 2 * ((2 + (2 ** (1 / 2))) ** (1 / 2))\n",
    "second_side = 1 - 1 / 4 * (2 + (2 ** (1 / 2)))\n",
    "possible_moves = [Vec2(0, 0)\n",
    "    , Vec2(1, 0), Vec2(0, 1), Vec2(-1, 0), Vec2(0, -1)\n",
    "    , Vec2(1, 1), Vec2(-1, -1), Vec2(-1, 1), Vec2(1, -1)\n",
    "    , Vec2(cos_22dot5, second_side), Vec2(-cos_22dot5, -second_side), Vec2(-cos_22dot5, second_side),\n",
    "                  Vec2(cos_22dot5, -second_side)\n",
    "    , Vec2(second_side, cos_22dot5), Vec2(-second_side, -cos_22dot5), Vec2(-second_side, cos_22dot5),\n",
    "                  Vec2(second_side, -cos_22dot5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:38:30.449790Z",
     "start_time": "2025-01-17T02:38:30.442212Z"
    },
    "id": "1YhER9eknNSY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:38:30.903112Z",
     "start_time": "2025-01-17T02:38:30.893602Z"
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:38:31.616290Z",
     "start_time": "2025-01-17T02:38:31.604956Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:42:55.956464Z",
     "start_time": "2025-01-17T02:42:55.925772Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainingParams:\n",
    "    def __init__(self, batch_size=128, gamma=0.99, epsilon_start=0.9, epsilon_end=0.02, epsilon_decay=10000,\n",
    "                 learning_rate=1e-4, update_rate=0.005, replay_memory_size: int = 1e4):\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_rate = update_rate\n",
    "        self.replay_memory_size = replay_memory_size\n",
    "\n",
    "\n",
    "class RewardData:\n",
    "    def __init__(self, state, own_goal_post, opp_goal_post):\n",
    "        self.state = state\n",
    "        self.own_goal_post = own_goal_post\n",
    "        self.opp_goal_post = opp_goal_post\n",
    "\n",
    "    def get_as_array(self):\n",
    "        return self.state + [self.own_goal_post[0], self.own_goal_post[1], self.own_goal_post[2], self.opp_goal_post[0],\n",
    "                             self.opp_goal_post[1], self.opp_goal_post[2]]\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, n_observations, n_actions, training_params: TrainingParams, reward_system, add_random_pos: bool,\n",
    "                 name: str):\n",
    "        self.n_observations = n_observations\n",
    "        self.n_actions = n_actions\n",
    "        self.training_params = training_params\n",
    "\n",
    "        self.policy_net = DQN(n_observations, n_actions).to(device)\n",
    "        self.target_net = DQN(n_observations, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.training_params.learning_rate, amsgrad=True)\n",
    "        self.memory = ReplayMemory(self.training_params.replay_memory_size)\n",
    "        self.steps_done = 0\n",
    "        self.reward_system = reward_system\n",
    "        self.add_random_pos = add_random_pos\n",
    "        self.name = name\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.training_params.epsilon_end + (\n",
    "                self.training_params.epsilon_start - self.training_params.epsilon_end) * \\\n",
    "                        math.exp(-1. * self.steps_done / self.training_params.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(len(possible_moves))]], device=device)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.training_params.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.training_params.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None]).to(device)\n",
    "        state_batch = torch.cat(batch.state).to(device)\n",
    "        action_batch = torch.cat(batch.action).to(device)\n",
    "        reward_batch = torch.cat(batch.reward).to(device)\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(self.training_params.batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(2)[0].view(\n",
    "                self.training_params.batch_size)\n",
    "        expected_state_action_values = (next_state_values * self.training_params.gamma) + reward_batch\n",
    "\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1)).to(device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def calculate_reward(self, reward_data: RewardData):\n",
    "        return self.reward_system(reward_data)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'memory': self.memory,\n",
    "            'steps_done': self.steps_done\n",
    "        }, filename)\n",
    "        print(f\"saved to: {filename}\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        data = torch.load(filename)\n",
    "        self.policy_net.load_state_dict(data['policy_net'])\n",
    "        self.target_net.load_state_dict(data['target_net'])\n",
    "        self.optimizer.load_state_dict(data['optimizer'])\n",
    "        self.memory = data['memory']\n",
    "        self.steps_done = data['steps_done']\n",
    "\n",
    "    def update_networks(self):\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = self.target_net.state_dict()\n",
    "        policy_net_state_dict = self.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * self.training_params.update_rate + \\\n",
    "                                         target_net_state_dict[key] * (1 - self.training_params.update_rate)\n",
    "        self.target_net.load_state_dict(target_net_state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:42:56.359887Z",
     "start_time": "2025-01-17T02:42:56.325528Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, env: Haxball, agent1: Agent, agent2: Agent, episode_length: int):\n",
    "        self.env = env\n",
    "        self.agent1 = agent1\n",
    "        self.agent2 = agent2\n",
    "        self.episode_length = episode_length\n",
    "        self.info = self.env.info()\n",
    "        self.goal_post1 = (\n",
    "        self.info[\"own_postTop_pos\"].x, self.info[\"own_postTop_pos\"].y, self.info[\"own_postBot_pos\"].y)\n",
    "        self.goal_post2 = (\n",
    "        self.info[\"opp_postTop_pos\"].x, self.info[\"opp_postTop_pos\"].y, self.info[\"opp_postBot_pos\"].y)\n",
    "        self.team1_goal_post_own = self.goal_post1\n",
    "        self.team1_goal_post_opp = self.goal_post2\n",
    "        self.team2_goal_post_own = self.goal_post2\n",
    "        self.team2_goal_post_opp = self.goal_post1\n",
    "        self.team1_random_place = None\n",
    "        self.team2_random_place = None\n",
    "        self.pitch_size = self.env.info()[\"pitch_size\"]\n",
    "        self.begin_area_x = 60\n",
    "        self.begin_area_y = 20\n",
    "\n",
    "    def random_place_on_pitch(self):\n",
    "        return ((self.pitch_size[0] - self.begin_area_x) * np.random.random_sample() + self.begin_area_x,\n",
    "                (self.pitch_size[1] - 2 * self.begin_area_y) * np.random.random_sample() + self.begin_area_y)\n",
    "\n",
    "    def train_players(self, number_of_episodes):\n",
    "        try:\n",
    "            episodes_data = []\n",
    "            print(f\"Training for {number_of_episodes} episodes started\")\n",
    "            for _ in tqdm(range(number_of_episodes)):\n",
    "                time, winning_team = self.one_game()\n",
    "                episodes_data.append((time, winning_team))\n",
    "            print(f\"Training for {number_of_episodes} episodes finished\")\n",
    "            return episodes_data\n",
    "        except BaseException:\n",
    "            print(\"Training failed, saving data\")\n",
    "            self.save_agents()\n",
    "\n",
    "    def one_game(self):\n",
    "        self.env.reset()\n",
    "        (state1_env, _, _), (state2_env, _, _), done = self.env.state()\n",
    "        if self.agent1.add_random_pos:\n",
    "            self.team1_random_place = self.random_place_on_pitch()\n",
    "            state1_env.append(self.team1_random_place[0])\n",
    "            state1_env.append(self.team1_random_place[1])\n",
    "        if self.agent2.add_random_pos:\n",
    "            self.team2_random_place = self.random_place_on_pitch()\n",
    "            state2_env.append(self.team2_random_place[0])\n",
    "            state2_env.append(self.team2_random_place[1])\n",
    "        for t in range(self.episode_length):\n",
    "            state1 = RewardData(state1_env, self.team1_goal_post_own, self.team1_goal_post_opp).get_as_array()\n",
    "            state2 = RewardData(state2_env, self.team2_goal_post_own, self.team2_goal_post_opp).get_as_array()\n",
    "            step_data_1 = torch.tensor([state1], dtype=torch.float32, device=device)\n",
    "            step_data_2 = torch.tensor([state2], dtype=torch.float32, device=device)\n",
    "\n",
    "            action1_index = self.agent1.select_action(step_data_1)\n",
    "            action2_index = self.agent2.select_action(step_data_2)\n",
    "            action1 = possible_moves[action1_index.item()]\n",
    "            action2 = possible_moves[action2_index.item()]\n",
    "            (next_state1_env, goal_1, _), (next_state2_env, goal_2, _), done = self.env.step(action1, action2)\n",
    "\n",
    "            if self.agent1.add_random_pos:\n",
    "                next_state1_env.append(self.team1_random_place[0])\n",
    "                next_state1_env.append(self.team1_random_place[1])\n",
    "            if self.agent2.add_random_pos:\n",
    "                next_state2_env.append(self.team2_random_place[0])\n",
    "                next_state2_env.append(self.team2_random_place[1])\n",
    "\n",
    "            next_state1 = RewardData(next_state1_env, self.team1_goal_post_own, self.team1_goal_post_opp)\n",
    "            next_state2 = RewardData(next_state2_env, self.team2_goal_post_own, self.team2_goal_post_opp)\n",
    "            next_step_data_1 = torch.tensor([next_state1.get_as_array()], dtype=torch.float32, device=device).unsqueeze(\n",
    "                0)\n",
    "            next_step_data_2 = torch.tensor([next_state2.get_as_array()], dtype=torch.float32, device=device).unsqueeze(\n",
    "                0)\n",
    "            reward_1 = torch.tensor([[self.agent1.calculate_reward(next_state1)]], dtype=torch.float32, device=device)\n",
    "            reward_2 = torch.tensor([[self.agent2.calculate_reward(next_state2)]], dtype=torch.float32, device=device)\n",
    "\n",
    "            # Store the transition in memory\n",
    "            self.agent1.memory.push(step_data_1, action1_index, next_step_data_1, reward_1)\n",
    "            self.agent2.memory.push(step_data_2, action2_index, next_step_data_2, reward_2)\n",
    "\n",
    "            # Move to the next state\n",
    "            state1_env = next_state1_env\n",
    "            state2_env = next_state2_env\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            # print(t)\n",
    "            self.agent1.optimize_model()\n",
    "            self.agent2.optimize_model()\n",
    "\n",
    "            self.agent1.update_networks()\n",
    "            self.agent2.update_networks()\n",
    "\n",
    "            if done:\n",
    "                if goal_1:\n",
    "                    scoring_player = 1\n",
    "                else:\n",
    "                    scoring_player = 2\n",
    "                return t, scoring_player\n",
    "        return self.episode_length, 0\n",
    "\n",
    "    def test_agents(self, test_game_length, with_show_video=False):\n",
    "        self.env.reset()\n",
    "        (state1_env, _, _), (state2_env, _, _), done = self.env.state()\n",
    "        if self.agent1.add_random_pos:\n",
    "            self.team1_random_place = self.random_place_on_pitch()\n",
    "            state1_env.append(self.team1_random_place[0])\n",
    "            state1_env.append(self.team1_random_place[1])\n",
    "        if self.agent2.add_random_pos:\n",
    "            self.team2_random_place = self.random_place_on_pitch()\n",
    "            state2_env.append(self.team2_random_place[0])\n",
    "            state2_env.append(self.team2_random_place[1])\n",
    "        for t in range(test_game_length):\n",
    "            state1 = RewardData(state1_env, self.team1_goal_post_own, self.team1_goal_post_opp).get_as_array()\n",
    "            state2 = RewardData(state2_env, self.team2_goal_post_own, self.team2_goal_post_opp).get_as_array()\n",
    "            step_data_1 = torch.tensor([state1], dtype=torch.float32, device=device)\n",
    "            step_data_2 = torch.tensor([state2], dtype=torch.float32, device=device)\n",
    "\n",
    "            action1_index = self.agent1.select_action(step_data_1)\n",
    "            action2_index = self.agent2.select_action(step_data_2)\n",
    "            action1 = possible_moves[action1_index.item()]\n",
    "            action2 = possible_moves[action2_index.item()]\n",
    "            (next_state1_env, goal_1, _), (next_state2_env, goal_2, _), done = self.env.step(action1, action2)\n",
    "\n",
    "            if self.agent1.add_random_pos:\n",
    "                next_state1_env.append(self.team1_random_place[0])\n",
    "                next_state1_env.append(self.team1_random_place[1])\n",
    "            if self.agent2.add_random_pos:\n",
    "                next_state2_env.append(self.team2_random_place[0])\n",
    "                next_state2_env.append(self.team2_random_place[1])\n",
    "\n",
    "            # Move to the next state\n",
    "            state1_env = next_state1_env\n",
    "            state2_env = next_state2_env\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        self.env.showReplay(custom_string=f\"{self.agent1.name}-{self.agent2.name}\", with_show_video=with_show_video)\n",
    "\n",
    "    def save_agents(self):\n",
    "        print(\"Saving agents\")\n",
    "        timestamp = datetime.now().strftime('%d-%m-%y--%H-%M-%S')\n",
    "        self.agent1.save(f\"{self.agent1.name}_1_{timestamp}.checkpoint\")\n",
    "        print(f\"Saved agent: {self.agent1.name}_1_{timestamp}.checkpoint\")\n",
    "        self.agent2.save(f\"{self.agent2.name}_2_{timestamp}.checkpoint\")\n",
    "        print(f\"Saved agent: {self.agent2.name}_2_{timestamp}.checkpoint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:42:56.858338Z",
     "start_time": "2025-01-17T02:42:56.850822Z"
    }
   },
   "outputs": [],
   "source": [
    "def distance(position1, position2):\n",
    "    return math.sqrt((position1[0] - position2[0]) ** 2 + (position1[1] - position2[1]) ** 2)\n",
    "\n",
    "\n",
    "def swerving_reward(reward_data: RewardData):  #requires 2 additional arguments in n_observations\n",
    "    ball_position = (reward_data.state[0], reward_data.state[1])\n",
    "    place_to_be = (reward_data.state[-2], reward_data.state[-1])\n",
    "    reward = 400 - distance(ball_position, place_to_be)\n",
    "    return reward\n",
    "\n",
    "\n",
    "def swerving_reward_with_penalty(reward_data: RewardData):  #requires 2 additional arguments in n_observations\n",
    "    ball_position = (reward_data.state[0], reward_data.state[1])\n",
    "    place_to_be = (reward_data.state[-2], reward_data.state[-1])\n",
    "    my_position = (reward_data.state[4], reward_data.state[5])\n",
    "    reward = 400 - distance(ball_position, place_to_be) - 5 * distance(ball_position, my_position)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:42:57.859586Z",
     "start_time": "2025-01-17T02:42:57.823742Z"
    }
   },
   "outputs": [],
   "source": [
    "env1 = Haxball()\n",
    "# Get the number of state observations\n",
    "(state, _, _), _, _ = env1.state()\n",
    "n_observations_swerving = len(state) + 8  #goalpost info = 6 + swerving_reward = 2\n",
    "n_actions_to_select = len(possible_moves)\n",
    "training_params_swerving1 = TrainingParams(batch_size=128, gamma=0.99, epsilon_start=0.9, epsilon_end=0.02,\n",
    "                                           epsilon_decay=10000, learning_rate=1e-4, update_rate=0.005,\n",
    "                                           replay_memory_size=10 ** 6)\n",
    "training_params_swerving2 = TrainingParams(batch_size=128, gamma=0.99, epsilon_start=0.9, epsilon_end=0.02,\n",
    "                                           epsilon_decay=10000, learning_rate=1e-4, update_rate=0.005,\n",
    "                                           replay_memory_size=10 ** 3)\n",
    "swerving_agent1 = Agent(n_observations_swerving, n_actions_to_select, training_params_swerving1, swerving_reward,\n",
    "                        add_random_pos=True, name=\"swerving_agent\")\n",
    "swerving_agent2 = Agent(n_observations_swerving, n_actions_to_select, training_params_swerving2, swerving_reward,\n",
    "                        add_random_pos=True, name=\"swerving_agent\")\n",
    "\n",
    "swerving_trainer = Trainer(env=env1, agent1=swerving_agent1, agent2=swerving_agent2, episode_length=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:42:58.934327Z",
     "start_time": "2025-01-17T02:42:58.929652Z"
    }
   },
   "outputs": [],
   "source": [
    "num_of_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:43:11.725652Z",
     "start_time": "2025-01-17T02:42:59.833847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 episodes started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/GUZW_W2/venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([128, 1, 128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1000/1000 [3:39:18<00:00, 13.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 episodes finished\n",
      "Saving agents\n",
      "saved to: swerving_agent_1_17-01-25--07-27-25.checkpoint\n",
      "Saved agent: swerving_agent_1_17-01-25--07-27-25.checkpoint\n",
      "saved to: swerving_agent_2_17-01-25--07-27-25.checkpoint\n",
      "Saved agent: swerving_agent_2_17-01-25--07-27-25.checkpoint\n"
     ]
    }
   ],
   "source": [
    "swerving_trainer.train_players(num_of_episodes)\n",
    "swerving_trainer.save_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swerving_trainer.test_agents(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:43:15.742805Z",
     "start_time": "2025-01-17T02:43:15.154684Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101692/1435209628.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(filename)\n"
     ]
    }
   ],
   "source": [
    "swerving_agent1 = Agent(n_observations_swerving, n_actions_to_select, training_params_swerving1, swerving_reward,\n",
    "                        add_random_pos=True, name=\"swerving_agent\")\n",
    "swerving_agent2 = Agent(n_observations_swerving, n_actions_to_select, training_params_swerving2, swerving_reward,\n",
    "                        add_random_pos=True, name=\"swerving_agent\")\n",
    "swerving_agent1.load(\"swerving_agent_1_17-01-25--03-40-52.checkpoint\")\n",
    "swerving_agent2.load(\"swerving_agent_2_17-01-25--03-40-52.checkpoint\")\n",
    "swerving_trainer.agent1 = swerving_agent1\n",
    "swerving_trainer.agent2 = swerving_agent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:43:35.599221Z",
     "start_time": "2025-01-17T02:43:29.478353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_swerving_agent-swerving_agent_17-01-25--03-43-30.mp4\n"
     ]
    }
   ],
   "source": [
    "swerving_trainer.test_agents(1800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
